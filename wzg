#kaggle比赛代码，预测3-11年级学生撰写摘要的内容和语言评分

#库+去警报
from typing import List
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import warnings
import logging
import os
import shutil
import json
import transformers
from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification
from transformers import DataCollatorWithPadding
from datasets import Dataset,load_dataset, load_from_disk
from transformers import TrainingArguments, Trainer
from datasets import load_metric, disable_progress_bar
from sklearn.metrics import mean_squared_error
import torch
from sklearn.model_selection import KFold, GroupKFold
from tqdm import tqdm
import gc
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer
from collections import Counter
import spacy
import re
import lightgbm as lgb
import torch.nn as nn
#不显示警报
warnings.simplefilter("ignore")
logging.disable(logging.ERROR)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
disable_progress_bar()
tqdm.pandas()
#设置随机数
def seed_everything(seed: int):
    import random, os
    import numpy as np
    import torch
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = True
seed_everything(seed=42)
#参数设置
class CFG:
    model_name="deberta-v3-large"
    learning_rate=1e-5
    weight_decay=0.02
    hidden_dropout_prob=0.007
    attention_probs_dropout_prob=0.0
    num_train_epochs=5
    n_splits=4
    batch_size=12
    random_seed=42
    save_steps=100
    max_length=1600
DATA_DIR = "/kaggle/input/commonlit-evaluate-student-summaries/"
prompts_train = pd.read_csv(DATA_DIR + "prompts_train.csv")
prompts_test = pd.read_csv(DATA_DIR + "prompts_test.csv")
summaries_train = pd.read_csv(DATA_DIR + "summaries_train.csv")
summaries_test = pd.read_csv(DATA_DIR + "summaries_test.csv")
sample_submission = pd.read_csv(DATA_DIR + "sample_submission.csv")
#评价指标
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    rmse = mean_squared_error(labels, predictions, squared=False)
    return {"rmse": rmse}
def compute_mcrmse(eval_pred):
    preds, labels = eval_pred
    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))
    mcrmse = np.mean(col_rmse)
    return {
        "content_rmse": col_rmse[0],
        "wording_rmse": col_rmse[1],
        "mcrmse": mcrmse,
    }
def compt_score(content_true, content_pred, wording_true, wording_pred):
    content_score = mean_squared_error(content_true, content_pred)**(1/2)
    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)
    return (content_score + wording_score)/2


!pip install "/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl"
from spellchecker import SpellChecker
#数据预处理函数
class Preprocessor:
    def __init__(self, 
                model_name: str,
                ) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(f"/kaggle/input/{model_name}")
        self.twd = TreebankWordDetokenizer()
        self.STOP_WORDS = set(stopwords.words('english'))
        
        self.spacy_ner_model = spacy.load('en_core_web_sm',)
        self.spellchecker = SpellChecker() 
        
    def word_overlap_count(self, row):
        """ intersection(prompt_text, text) """        
        def check_is_stop_word(word):
            return word in self.STOP_WORDS
        
        prompt_words = row['prompt_tokens']
        summary_words = row['summary_tokens']
        if self.STOP_WORDS:
            prompt_words = list(filter(check_is_stop_word, prompt_words))
            summary_words = list(filter(check_is_stop_word, summary_words))
        return len(set(prompt_words).intersection(set(summary_words)))
            
    def ngrams(self, token, n):
        # Use the zip function to help us generate n-grams
        # Concatentate the tokens into ngrams and return
        ngrams = zip(*[token[i:] for i in range(n)])
        return [" ".join(ngram) for ngram in ngrams]

    def ngram_co_occurrence(self, row, n: int) -> int:
        # Tokenize the original text and summary into words
        original_tokens = row['prompt_tokens']
        summary_tokens = row['summary_tokens']

        # Generate n-grams for the original text and summary
        original_ngrams = set(self.ngrams(original_tokens, n))
        summary_ngrams = set(self.ngrams(summary_tokens, n))

        # Calculate the number of common n-grams
        common_ngrams = original_ngrams.intersection(summary_ngrams)

        return len(common_ngrams)
    
    def ner_overlap_count(self, row, mode:str):
        model = self.spacy_ner_model
        def clean_ners(ner_list):
            return set([(ner[0].lower(), ner[1]) for ner in ner_list])
        prompt = model(row['prompt_text'])
        summary = model(row['text'])

        if "spacy" in str(model):
            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])
            summary_ner = set([(token.text, token.label_) for token in summary.ents])
        elif "stanza" in str(model):
            prompt_ner = set([(token.text, token.type) for token in prompt.ents])
            summary_ner = set([(token.text, token.type) for token in summary.ents])
        else:
            raise Exception("Model not supported")

        prompt_ner = clean_ners(prompt_ner)
        summary_ner = clean_ners(summary_ner)

        intersecting_ners = prompt_ner.intersection(summary_ner)
        
        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))
        
        if mode == "train":
            return ner_dict
        elif mode == "test":
            return {key: ner_dict.get(key) for key in self.ner_keys}

    
    def quotes_count(self, row):
        summary = row['text']
        text = row['prompt_text']
        quotes_from_summary = re.findall(r'"([^"]*)"', summary)
        if len(quotes_from_summary)>0:
            return [quote in text for quote in quotes_from_summary].count(True)
        else:
            return 0

    def spelling(self, text):
        
        wordlist=text.split()
        amount_miss = len(list(self.spellchecker.unknown(wordlist)))

        return amount_miss
    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:
        """dictionary update for pyspell checker and autocorrect"""
        self.spellchecker.word_frequency.load_words(tokens)
    def run(self, 
            prompts: pd.DataFrame,
            summaries:pd.DataFrame,
            mode:str
        ) -> pd.DataFrame:
        
        # before merge preprocess
        prompts["prompt_length"] = prompts["prompt_text"].apply(
            lambda x: len(word_tokenize(x))
        )
        prompts["prompt_tokens"] = prompts["prompt_text"].apply(
            lambda x: word_tokenize(x)
        )

        summaries["summary_length"] = summaries["text"].apply(
            lambda x: len(word_tokenize(x))
        )
        summaries["summary_tokens"] = summaries["text"].apply(
            lambda x: word_tokenize(x)
        )
        
        # Add prompt tokens into spelling checker dictionary
        prompts["prompt_tokens"].apply(
            lambda x: self.add_spelling_dictionary(x)
        )

        
        # count misspelling
        summaries["splling_err_num"] = summaries["text"].progress_apply(self.spelling)
        
        # merge prompts and summaries
        input_df = summaries.merge(prompts, how="left", on="prompt_id")

        # after merge preprocess
        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']
        
        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)
        input_df['bigram_overlap_count'] = input_df.progress_apply(
            self.ngram_co_occurrence,args=(2,), axis=1 
        )
        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)
        
        input_df['trigram_overlap_count'] = input_df.progress_apply(
            self.ngram_co_occurrence, args=(3,), axis=1
        )
        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)
    
        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)
        
        return input_df.drop(columns=["summary_tokens", "prompt_tokens"])

preprocessor = Preprocessor(model_name=CFG.model_name)
test = preprocessor.run(prompts_test, summaries_test, mode="train")#测试集预处理

#读取模型，获取多个模型预测结果
class ScoreRegressor:
    def __init__(self, 
                model_name: str,
                model_dir: str,
                inputs: List[str],
                target_cols: List[str],
                hidden_dropout_prob: float,
                attention_probs_dropout_prob: float,
                max_length: int,
                ):
        
        self.input_col = "input" # col name of model input after text concat sep token
        
        self.input_text_cols = inputs
        self.target_cols = target_cols
        self.model_name = model_name
        self.model_dir = model_dir
        self.max_length = max_length
        
        self.tokenizer = AutoTokenizer.from_pretrained(f"/kaggle/input/{model_name}")
        self.model_config = AutoConfig.from_pretrained(f"/kaggle/input/{model_name}")
        
        self.model_config.update({
            "hidden_dropout_prob": hidden_dropout_prob,
            "attention_probs_dropout_prob": attention_probs_dropout_prob,
            "num_labels": 2,
            "problem_type": "regression",
        })

        self.data_collator = DataCollatorWithPadding(
            tokenizer=self.tokenizer
        )

    def concatenate_with_sep_token(self, row):
        sep = " " + self.tokenizer.sep_token + " "        
        return sep.join(row[self.input_text_cols])

    def tokenize_function(self, examples: pd.DataFrame):
        labels = [examples["content"], examples["wording"]]
        tokenized = self.tokenizer(examples[self.input_col],
                        padding=False,
                        truncation=True,
                        max_length=self.max_length)
        return {
            **tokenized,
            "labels": labels,
        }
    
    def tokenize_function_test(self, examples: pd.DataFrame):
        tokenized = self.tokenizer(examples[self.input_col],
#                         padding="max_length",
                        padding=False,
                        truncation=True,
                        max_length=self.max_length)
        return tokenized
    def predict(self, 
                test_df: pd.DataFrame,
                batch_size: int,
                fold: int,
               ):
        test_df[self.input_col] = test_df.apply(self.concatenate_with_sep_token, axis=1)

        test_dataset = Dataset.from_pandas(test_df[[self.input_col]], preserve_index=False) 
        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)

        model = AutoModelForSequenceClassification.from_pretrained(f"/kaggle/input/{self.model_name}")
        model.load_state_dict(torch.load(f"{self.model_dir}/{self.model_name}.pth"))
        model.eval()
        
        model_fold_dir = '/kaggle/working/'
        test_args = TrainingArguments(
            output_dir=model_fold_dir,
            do_train=False,
            do_predict=True,
            per_device_eval_batch_size=batch_size,
            dataloader_drop_last=False,
#             fp16=True,
            auto_find_batch_size=True,
        )
        infer_content = Trainer(
                      model = model, 
                      tokenizer=self.tokenizer,
                      data_collator=self.data_collator,
                      args = test_args)
        preds = infer_content.predict(test_tokenized_dataset)[0]
        pred_df = pd.DataFrame(
            preds, 
            columns=[
                f"content_pred", 
                f"wording_pred"
           ]
        )
        model.cpu()
        del model
        gc.collect()
        torch.cuda.empty_cache()
        return pred_df
#测试集
def predict(
    test_df: pd.DataFrame,
    mode,
    name,
    targets:List[str],
    inputs: List[str],
    save_each_model: bool,
    n_splits: int,
    batch_size: int,
    model_name: str,
    hidden_dropout_prob: float,
    attention_probs_dropout_prob: float,
    max_length : int
    ):
 #交叉验证
    
    columns = list(test_df.columns.values)

    for fold in range(n_splits):
        print(f"fold {fold}:")
        
        model_dir =  f"/kaggle/input/{name}/{model_name}/fold_{fold}"

        csr = ScoreRegressor(
            model_name=model_name,
            target_cols=targets,
            inputs= inputs,
            model_dir = model_dir, 
            hidden_dropout_prob=hidden_dropout_prob,
            attention_probs_dropout_prob=attention_probs_dropout_prob,
            max_length=max_length,
           )
        
        pred_df = csr.predict(
            test_df=test_df, 
            batch_size=batch_size,
            fold=fold
        )

        test_df[f"content_{mode}_pred_{fold}"] = pred_df[f"content_pred"].values
        test_df[f"wording_{mode}_pred_{fold}"] = pred_df[f"wording_pred"].values

    test_df[f"content_{mode}_pred"] = test_df[[f"content_{mode}_pred_{fold}" for fold in range(n_splits)]].mean(axis=1)
    test_df[f"wording_{mode}_pred"] = test_df[[f"wording_{mode}_pred_{fold}" for fold in range(n_splits)]].mean(axis=1)
    
    return test_df[columns + [f"content_{mode}_pred", f"wording_{mode}_pred"]]
targets = ["wording", "content"]
input_cols = ["prompt_title", "prompt_question", "text"]
model_cfg = CFG

test = predict(
    test,
    mode="multi",
    name='large-1600-frezz23-2model',
    targets=targets,
    inputs=input_cols,
    save_each_model=False,
    batch_size=12,
    n_splits=CFG.n_splits,
    model_name='deberta-v3-large',
    hidden_dropout_prob=model_cfg.hidden_dropout_prob,
    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,
    max_length=1600
)
test = predict(
    test,
    mode="multi1",
    name='base-1600-frezz6-2model',
    targets=targets,
    inputs=input_cols,
    save_each_model=False,
    batch_size=12,
    n_splits=CFG.n_splits,
    model_name='deberta-v3-base',
    hidden_dropout_prob=model_cfg.hidden_dropout_prob,
    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,
    max_length=1600
)

# test = predict(
#     test,
#     mode="multi1",
#     name='large-1600-frezz6-2model',
#     targets=targets,
#     inputs=input_cols,
#     save_each_model=False,
#     batch_size=12,
#     n_splits=CFG.n_splits,
#     model_name='deberta-v3-large',
#     hidden_dropout_prob=model_cfg.hidden_dropout_prob,
#     attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,
#     max_length=1600
# )
# test = predict(
#     test,
#     mode="multi2",
#     name='large-512-frezz23-2model',
#     targets=targets,
#     inputs=input_cols,
#     save_each_model=False,
#     batch_size=12,
#     n_splits=CFG.n_splits,
#     model_name='deberta-v3-large',
#     hidden_dropout_prob=model_cfg.hidden_dropout_prob,
#     attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,
#     max_length=512
# )
test = predict(
    test,
    mode="multi2",
    name='deberta-large-1600-frezz23-2model',
    targets=targets,
    inputs=input_cols,
    save_each_model=False,
    batch_size=12,
    n_splits=CFG.n_splits,
    model_name='deberta-large',
    hidden_dropout_prob=model_cfg.hidden_dropout_prob,
    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,
    max_length=1600
)
test = predict(
    test,
    mode="multi3",
    name='large-squad2-1600-frezz23-2model',
    targets=targets,
    inputs=input_cols,
    save_each_model=False,
    batch_size=12,
    n_splits=CFG.n_splits,
    model_name='deberta-v3-large-squad2',
    hidden_dropout_prob=model_cfg.hidden_dropout_prob,
    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,
    max_length=1600
)
#读取训练集
train=pd.read_csv('/kaggle/input/large-1600-frezz23-2model/deberta-v3-large_train.csv')#读取训练集
train1=pd.read_csv('/kaggle/input/base-1600-frezz6-2model/deberta-v3-base_train.csv')#读取训练集
train2=pd.read_csv('/kaggle/input/deberta-large-1600-frezz23-2model/deberta-large_train.csv')#读取训练集
train3=pd.read_csv('/kaggle/input/large-squad2-1600-frezz23-2model/deberta-v3-large-squad2_train.csv')#读取训练集
# train1=pd.read_csv('/kaggle/input/large-1600-frezz6-2model/deberta-v3-large_train.csv')#读取训练集
# train2=pd.read_csv('/kaggle/input/large-512-frezz23-2model/deberta-v3-large_train.csv')#读取训练集
train["content_multi1_pred"]=train1["content_multi_pred"];train["wording_multi1_pred"]=train1["wording_multi_pred"]
train["content_multi2_pred"]=train2["content_multi_pred"];train["wording_multi2_pred"]=train2["wording_multi_pred"]
train["content_multi3_pred"]=train3["content_multi_pred"];train["wording_multi3_pred"]=train3["wording_multi_pred"]






#调用xgb使用stacking策略完成集成学习（输出训练、验证集预测结果）
from sklearn.svm import SVR
import xgboost as xgb
targets = ["content", "wording"]
drop_columns = ["fold", "student_id", "prompt_id", "text", 
                "prompt_question", "prompt_title",  
                "prompt_text"
               ] + targets
model_dict = {}

for target in targets:
    models = []
    
    for fold in range(CFG.n_splits):

        X_train_cv = train[train["fold"] != fold].drop(columns=drop_columns)
        y_train_cv = train[train["fold"] != fold][target]

        X_eval_cv = train[train["fold"] == fold].drop(columns=drop_columns)
        y_eval_cv = train[train["fold"] == fold][target]

        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)
        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)

        params = {
            'objective': 'reg:squarederror',
                    'eval_metric': 'rmse',
                    'tree_method': 'gpu_hist',  # 使用GPU加速
                'n_estimators': 494, 'max_depth': 4, 'learning_rate': 0.014372233029418346, 'gamma': 5.439111143133857e-05, 'subsample': 0.534121265920542, 'colsample_bytree': 0.5441551218521704
                }

        evaluation_results = {}
        model =xgb.XGBRegressor(**params)
        model.fit(X_train_cv, y_train_cv)
        models.append(model)
    
    model_dict[target] = models
# cv
rmses = []

for target in targets:
    models = model_dict[target]

    preds = []
    trues = []
    
    for fold, model in enumerate(models):
        X_eval_cv = train[train["fold"] == fold].drop(columns=drop_columns)
        y_eval_cv = train[train["fold"] == fold][target]

        pred = model.predict(X_eval_cv)

        trues.extend(y_eval_cv)
        preds.extend(pred)
        
    rmse = np.sqrt(mean_squared_error(trues, preds))
    print(f"{target}_rmse : {rmse}")
    rmses = rmses + [rmse]

print(f"mcrmse : {sum(rmses) / len(rmses)}")





#集成模型参数寻优
# #调参
# # Best params: {'n_estimators': 494, 'max_depth': 4, 'learning_rate': 0.014372233029418346, 'gamma': 5.439111143133857e-05, 'subsample': 0.534121265920542, 'colsample_bytree': 0.5441551218521704}
# # Best loss: 0.48390252625434005
# import optuna
# import optuna.logging
# from sklearn.svm import SVR
# import xgboost as xgb
# # 在训练代码开始时设置日志级别
# optuna.logging.set_verbosity(optuna.logging.WARNING)  # 只打印警告和错误信息
# # 设置 LightGBM 的日志级别
# logging.getLogger('lightgbm').setLevel(logging.WARNING)  # 只打印警告和错误信息
# def objective(trial):
#     targets = ["content", "wording"]
#     drop_columns = ["fold", "student_id", "prompt_id", "text", 
#                     "prompt_question", "prompt_title", 
#                     "prompt_text"
#                    ] + targets
#     model_dict = {}

#     for target in targets:
#         models = []

#         for fold in range(CFG.n_splits):

#             X_train_cv = train[train["fold"] != fold].drop(columns=drop_columns)
#             y_train_cv = train[train["fold"] != fold][target]

#             X_eval_cv = train[train["fold"] == fold].drop(columns=drop_columns)
#             y_eval_cv = train[train["fold"] == fold][target]

#             dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)
#             dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)

# #             params = {
# #                       "C":trial.suggest_loguniform('C', 1e-5, 1e5),
# #                     "epsilon" :trial.suggest_loguniform('epsilon', 1e-5, 1e0),
# #                     "kernel" :trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),
# #                     "degree":trial.suggest_int('degree', 1, 5)
# #                       }
#             params = {
#                     'objective': 'reg:squarederror',
#                     'eval_metric': 'rmse',
#                     'tree_method': 'gpu_hist',  # 使用GPU加速
#                     'n_estimators': trial.suggest_int('n_estimators', 50, 1000),
#                     'max_depth': trial.suggest_int('max_depth', 3, 10),
#                     'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),
#                     'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),
#                     'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
#                     'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
#                 }

#             evaluation_results = {}
#             model = xgb.XGBRegressor(**params)
#             model.fit(X_train_cv, y_train_cv)
#             models.append(model)

#         model_dict[target] = models
#     # cv
#     rmses = []

#     for target in targets:
#         models = model_dict[target]

#         preds = []
#         trues = []

#         for fold, model in enumerate(models):
#             X_eval_cv = train[train["fold"] == fold].drop(columns=drop_columns)
#             y_eval_cv = train[train["fold"] == fold][target]

#             pred = model.predict(X_eval_cv)

#             trues.extend(y_eval_cv)
#             preds.extend(pred)
#         rmse = np.sqrt(mean_squared_error(trues, preds))
#         print(f"{target}_rmse : {rmse}")
#         rmses = rmses + [rmse]
#     print(f"mcrmse : {sum(rmses) / len(rmses)}")
#     return sum(rmses) / len(rmses)
# study = optuna.create_study(direction='minimize')  # 最小化 RMSE
# study.optimize(objective, n_trials=100)  # 运行 100 个试验
# best_params = study.best_params
# print("Best params:", best_params)
# # 获取最佳的损失值
# best_loss = study.best_value
# print("Best loss:", best_loss)




#输出测试集结果
drop_columns = [
                #"fold", 
                "student_id", "prompt_id", "text", 
                "prompt_question", "prompt_title", 
                "prompt_text"
               ]
pred_dict = {}
for target in targets:
    models = model_dict[target]
    preds = []

    for fold, model in enumerate(models):
        X_eval_cv = test.drop(columns=drop_columns)
        pred = model.predict(X_eval_cv)
        preds.append(pred)
    
    pred_dict[target] = preds
for target in targets:
    preds = pred_dict[target]
    for i, pred in enumerate(preds):
        test[f"{target}_pred_{i}"] = pred

    test[target] = test[[f"{target}_pred_{fold}" for fold in range(CFG.n_splits)]].mean(axis=1)
test[["student_id", "content", "wording"]].to_csv("submission.csv", index=False)
test[["student_id", "content", "wording"]]
